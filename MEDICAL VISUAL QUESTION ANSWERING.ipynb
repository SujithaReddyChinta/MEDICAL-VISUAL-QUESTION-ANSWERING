{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings,gensim\n",
    "warnings.filterwarnings('ignore')\n",
    "model_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "model_w2v = gensim.models.KeyedVectors.load_word2vec_format(model_path,\n",
    "binary=True) # Loading the model using genism\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import copy\n",
    "from random import shuffle, seed\n",
    "import sys\n",
    "import os.path\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pdb\n",
    "import string\n",
    "import h5py\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import gensim\n",
    "import json\n",
    "import re\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feat(doc):\n",
    "    feat = []\n",
    "    for word in doc:\n",
    "    try:\n",
    "    feat.append(model_w2v[word])\n",
    "    except:\n",
    "    pass\n",
    " return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return [i for i in re.split(r\"([-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=])\", sentence) if i!=''\n",
    "and i!=' ' and i!='\\n'];\n",
    "def prepro_question(imgs, method):\n",
    " # preprocess all the question\n",
    " print('example processed tokens:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f48ae",
   "metadata": {},
   "source": [
    " Input: The question from trainset.json\n",
    " Performs tokenization and lowering of the question\n",
    " Output: Embedded version of the question\n",
    " Note that: We have still not padded the questions, just for information each of the word will be a 300 Dimensional Vector,\n",
    " Hence, the word vector (which will be obtained after padding) will be a (21,300) Dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b23a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,img in enumerate(imgs):\n",
    "    s = img['question'].lower()\n",
    "    if method == 'nltk':\n",
    "        txt = word_tokenize(str(s).lower())\n",
    "    else:\n",
    "        txt = tokenize(s)\n",
    "    img['processed_tokens'] = txt\n",
    "    if i < 10: print(txt)\n",
    "        if i % 1000 == 0:\n",
    "            sys.stdout.write(\"processing %d/%d (%.2f%% done) \\r\" % (i, len(img),\n",
    "        i*100.0/len(imgs)) )\n",
    "        sys.stdout.flush()\n",
    "return imgs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_answers(imgs, num_ans):\n",
    " \"\"\"\n",
    " Print the questions and returns the time, one answer is repeated\n",
    " \"\"\"\n",
    " counts = {}\n",
    "    for img in imgs:\n",
    "    try:\n",
    "    ans = img['answer'].lower() # If the string is a number, it would result into error 27\n",
    "    except :\n",
    "    ans = str(img['answer'])\n",
    "    counts[ans] = counts.get(ans, 0) + 1\n",
    "    cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
    "    print('top answer and their counts:')\n",
    "    print('\\n'.join(map(str,cw[:20])))\n",
    "\n",
    "    vocab = []\n",
    "    for i in range(min(num_ans,len(cw))):\n",
    "        vocab.append(cw[i][1])\n",
    "return vocab[:num_ans] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec929bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_question(imgs, atoi):\n",
    "    new_imgs = []\n",
    "    for i, img in enumerate(imgs):\n",
    "        new_imgs.append(img)\n",
    "        print('question number reduce from %d to %d '%(len(imgs), len(new_imgs)))\n",
    "return new_imgs\n",
    "manualMap = { 'none': '0', 'zero': '0', 'one': '1', 'two': '2', 'three':\n",
    " '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7',\n",
    " 'eight': '8', 'nine': '9', 'ten': '10' }\n",
    "imgs_train = json.load(open('trainset.json' , 'r')) # Unnnecesarily, i have used the same\n",
    "file two times\n",
    "28\n",
    "num_ans = 1000\n",
    "top_ans = get_top_answers(imgs_train, num_ans)\n",
    "atoi = {w:i for i,w in enumerate(top_ans)} # Word : Count\n",
    "itoa = {i:w for i,w in enumerate(top_ans)} # Count : Word\n",
    "feat_dim = 300 # 300 Dimensional Vector\n",
    "imgs_data_train = json.load(open('trainset.json' , 'r')) # trainset.json\n",
    "num_ans = 10 # Even 1 should work fine, but I had taken reference from COCO\n",
    "dataset, and hence, 10 (10 represents the top 10 answers to a picture)\n",
    "method = 'nltk'\n",
    "max_length = 21 # Max Length of the question\n",
    "dir_path = \"QA\" # The path where we will be storing .h5 file\n",
    "N = len(imgs_data_train)\n",
    "image_path = 'VQA_RAD Image Folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ba1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data():\n",
    "img_path = image_path+img['image_name']\n",
    "\n",
    "     for i,img in enumerate(imgs_data_train):\n",
    " #print('X' , img['ques_id'])\n",
    " \n",
    "    s = img['question']\n",
    "    print(i,s) # Print the number and the question\n",
    "     if method == 'nltk':\n",
    "    try:\n",
    "    txt = word_tokenize(str(s).lower())\n",
    "    except :\n",
    "    txt = str(s)\n",
    "\n",
    "    else:\n",
    "    txt = tokenize(s)\n",
    "    img['processed_tokens'] = txt\n",
    "    question_id = img['qid']\n",
    "    feat = np.array(extract_feat(img['processed_tokens']))\n",
    "    label_arrays = np.zeros((1, max_length, feat_dim), dtype='float32')\n",
    "    label_length = min(max_length, len(feat)) # record the length of this sequence\n",
    "    label_arrays[0, :label_length, :] = feat\n",
    "    try:\n",
    "    ans_arrays = atoi[img['answer'].lower()]\n",
    "    except :\n",
    "    ans_arrays = atoi[str(img['answer'])]\n",
    "    f = h5py.File(os.path.join( dir_path , str(question_id) + '.h5'), \"w\")\n",
    "    f.create_dataset(\"ques_train\", dtype='float32', data=label_arrays)\n",
    "    f.create_dataset(\"answers\", dtype='uint32', data=ans_arrays)\n",
    "    f.close()\n",
    "return data = save_data() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cfa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import copy\n",
    "from random import shuffle, seed\n",
    "import sys\n",
    "import os.path\n",
    "30\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pdb\n",
    "import string\n",
    "import h5py\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import json\n",
    "import re\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "dropout_rate = 0.4\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feat(doc):\n",
    "    feat = []\n",
    "    for word in doc:\n",
    "    try:\n",
    "    feat.append(model_w2v[word])\n",
    "\n",
    "    except:\n",
    "    pass\n",
    " return feat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return [i for i in re.split(r\"([-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=])\", sentence) if i!=''\n",
    "and i!=' ' and i!='\\n'];\n",
    "def prepro_question(imgs, method):\n",
    "\n",
    " # preprocess all the question\n",
    " print('example processed tokens:')\n",
    "    for i,img in enumerate(imgs):\n",
    "    s = img['question'].lower()\n",
    "    if method == 'nltk':\n",
    "        txt = word_tokenize(str(s).lower())\n",
    "    else:\n",
    "        txt = tokenize(s)\n",
    "    img['processed_tokens'] = txt\n",
    "    if i < 10: print(txt)\n",
    "        if i % 1000 == 0:\n",
    "            sys.stdout.write(\"processing %d/%d (%.2f%% done) \\r\" % (i, len(img),\n",
    "i*100.0/len(imgs)) )\n",
    "sys.stdout.flush()\n",
    "return imgs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_answers(imgs, num_ans):\n",
    "    counts = {}\n",
    "    for img in imgs:\n",
    "\n",
    "    try:\n",
    "    ans = img['answer'].lower()\n",
    "    except :\n",
    "    ans = str(img['answer'])\n",
    "    counts[ans] = counts.get(ans, 0) + 1 # Frequency count\n",
    "    cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
    " # print('top answer and their counts:')\n",
    " # print('\\n'.join(map(str,cw[:20])))\n",
    "\n",
    "    vocab = []\n",
    "    for i in range(min(num_ans,len(cw))):\n",
    "    vocab.append(cw[i][1])\n",
    "return vocab[:num_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1819f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_question(imgs, atoi):\n",
    "    new_imgs = []\n",
    "    for i, img in enumerate(imgs):\n",
    "    new_imgs.append(img)\n",
    "    print('question number reduce from %d to %d '%(len(imgs), len(new_imgs)))\n",
    "return new_imgs\n",
    "def image_layer(input_shape):\n",
    "    base_model = tf.keras.applications.VGG16(input_shape=input_shape,include_top=False,weights='imagenet')\n",
    "    base_model.trainable = False # Do not train it\n",
    "\n",
    "    x = base_model.layers[-2].output # Shape would be (28*28*512)\n",
    "    x = tf.reshape(x , [-1,x.shape[2]*x.shape[1] , x.shape[3]]) # Shape would be (1,784,512)\n",
    "    x = tf.keras.layers.Dense(1024) # This step can be found out in the slides, that after feature extraction, they are connecting a dense layer, slide - 6 (Transforminto a same size vector)\n",
    "return x\n",
    "def vgg_preprocessing(model,image):\n",
    "return model(image)\n",
    "def load_data():\n",
    "    images = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    ids = []\n",
    "\n",
    " #print(start,end)\n",
    " #arrs = np.random.randint(0,len(imgs_data_train),batch)\n",
    " #data = [imgs_data_train[i] for i in arrs]\n",
    "data = imgs_data_train # trainset.json\n",
    "model = image_layer(input_shape = (448,448,3)) # Making VGG16 Model\n",
    " for i,img in enumerate(data):\n",
    "\n",
    "    img_path = img['image_name'] # Image Name\n",
    "    question_id = img['qid'] # Question id\n",
    "\n",
    " #label_arrays = np.zeros((1, max_length, feat_dim), dtype='float32') #\n",
    "Somethings are taken directly from\n",
    "\n",
    " with h5py.File(os.path.join(dir_path,str(question_id) + '.h5'),'r') as hf:\n",
    "    question = hf['.']['ques_train'][()] # Embedded question\n",
    "    answer = hf['.']['answers'][()] # Embedded answer\n",
    "\n",
    "    image = cv2.imread(os.path.join('VQA_RAD Image Folder',img_path) ,\n",
    "cv2.IMREAD_COLOR) # Reading the image\n",
    "    image = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image , (448,448)) # Reshape\n",
    " #image = vgg_preprocessing(model,image)\n",
    " # Apply VGG16 Preprocessings\n",
    "    images.append(image)\n",
    "    questions.append(np.array(question))\n",
    "    answers.append(np.array(answer))\n",
    "    ids.append(question_id)\n",
    "    if i%100==0:\n",
    "        print(\"Processed =>\",i,' which is',round(100*i/len(data),2),'%')\n",
    "\n",
    "questions = np.reshape(np.array(questions) , [-1,max_length,feat_dim])\n",
    "return (np.array(images) , questions ,np.array(answers) , np.array(ids))\n",
    "imgs_train = json.load(open('trainset.json' , 'r'))\n",
    "num_ans = 1000\n",
    "top_ans = get_top_answers(imgs_train, num_ans )\n",
    "atoi = {w:i for i,w in enumerate(top_ans)}\n",
    "itoa = {i:w for i,w in enumerate(top_ans)}\n",
    "feat_dim = 300\n",
    "imgs_data_train = json.load(open('trainset.json' , 'r'))\n",
    "num_ans = 10\n",
    "method = 'nltk'\n",
    "max_length = 21\n",
    "dir_path = \"QA\"\n",
    "N = len(imgs_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cedc6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras,h5py\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import keras.activations\n",
    "import keras.backend as kbe\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.layers as layers\n",
    "from keras.layers import Activation, Add, Concatenate, Conv1D, Dense, Dropout,\n",
    "Embedding, Softmax\n",
    "from keras.layers import Input, GlobalMaxPooling1D, Lambda, Multiply, RepeatVector,Reshape\n",
    "from keras.layers import BatchNormalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4700a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "''' These parameters are for some of the previous attempts, so ignore it, the main part is :\n",
    "datagen = load_data(), and I don't want to remove all of these,\n",
    "because indeed there can be some ideas coming out from these lines of code'''\n",
    "embed_size = 300\n",
    "q_len = 21\n",
    "height = 224\n",
    "width = 224\n",
    "lstm_units = 256\n",
    "attention_dim = 512\n",
    "num_output = 1000\n",
    "max_questions = 3064\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "articles = ['a', 'an', 'the']\n",
    "manualMap = { 'none': '0', 'zero': '00 'one': '1', 'two': '2', 'three':\n",
    " '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7',\n",
    " 'eight': '8', 'nine': '9', 'ten': '10' }\n",
    "datagen = load_data() # Load the Data\n",
    "images,questions,answers,ids =datagen[0],datagen[1],datagen[2],datagen[3]\n",
    "\n",
    "print(\"Images have a size of:\",images.shape)\n",
    "print(\"Questions have a size of:\",questions.shape)\n",
    "print(\"Answers have a size of:\",answers.shape)\n",
    "print(\"Ids have a size of:\",ids.shape)\n",
    "dir_path = r'QA' # The directory where the .h5 file for each entry is saved\n",
    "m = 100\n",
    "for i in range(images.shape[0]):\n",
    "    ans_array = answers[i]\n",
    "    image_array = images[i]\n",
    "    quest_array = questions[i]\n",
    "    question_id = ids[i]\n",
    "    f = h5py.File(os.path.join( dir_path , str(question_id) + '.h5'), \"w\") # Loading the 'h5\n",
    "file\n",
    "    f.create_dataset(\"ques_train\", dtype='float32', data=quest_array) # Question\n",
    "Embedding\n",
    "    f.create_dataset(\"image_vector\", dtype='float32', data=image_array) # Image\n",
    "Embedding (Not preprocessed)\n",
    "    f.create_dataset(\"answers\", dtype='uint32', data=ans_array) # Answers in embedded\n",
    "form\n",
    " f.close()\n",
    "    if i%m ==0:\n",
    "        print(\"Processed =>\", i,' total percentage =>', round(100*i/images.shape[0],2),' %')\n",
    "print(\"Your processing has been done\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,h5py\n",
    "# Image Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "''' Preprocessing with the VGG 16 Model '''\n",
    "model = tf.keras.applications.VGG16(include_top=False,weights='imagenet',\n",
    " input_shape=(448,448,3))\n",
    "#print(\"The Last layer\")\n",
    "last_layer = model.layers[-1].output # Last layer has an output layer of (14,14,512)\n",
    "model = Model(model.input,last_layer)\n",
    "model.trainable = False\n",
    "# print(model.summary())\n",
    "def extract_feature(image):\n",
    " ''' Preprocessing with VGG Netowrk'''\n",
    " image = model(image)\n",
    "return image # Shape is (196,512)\n",
    "''' The below model will convert (196,512) to (21,300) (i.e same as the dimension of word embedding) '''\n",
    "dimen_red = tf.keras.Sequential() # Use for converting (196,512) -> (21,300)\n",
    "dimen_red.add(tf.keras.layers.Conv2D(300,kernel_size=(1,1),input_shape= (14,14,512)))\n",
    "dimen_red.add(tf.keras.layers.Reshape((196,300)))\n",
    "dimen_red.add(tf.keras.layers.Permute((2,1))) # Reshaping about the axis, useful for\n",
    "applying the dense network\n",
    "dimen_red.add(tf.keras.layers.Dense(21))\n",
    "dimen_red.add(tf.keras.layers.Permute((2,1))) # Reshaping about the axis, useful for\n",
    "applying the dense network\n",
    "\n",
    "train_dir = r'QA/' # Containing .h5 file\n",
    "images = []\n",
    "ans = []\n",
    "ques = []\n",
    "count = 0\n",
    "content = os.listdir(train_dir)[:2300] # The GPU Memory became full after this, hence\n",
    "had to take just these much samples :(\n",
    "length = len(content)\n",
    "for i in content:\n",
    " # Reading the data\n",
    " file = h5py.File(train_dir+i)\n",
    " images.append(np.array(file['.']['image_vector'][()]))\n",
    " ans.append(np.array(file['.']['answers'][()]))\n",
    " ques.append(np.array(file['.']['ques_train'][()]))\n",
    " count+=1\n",
    " if count%100 == 0:\n",
    "    print(\"The count is:\",count,\"and the percentage proportion is:\",round(100*count/length,2),'%')\n",
    "images = tf.convert_to_tensor(np.array(images)) # For the GPU purpose\n",
    "ans = tf.convert_to_tensor(np.array(ans))\n",
    "ques = tf.convert_to_tensor(np.array(ques))\n",
    "l = []\n",
    "length = images.shape[0]\n",
    "for i,j in enumerate(images):\n",
    "    l.append(dimen_red(j)) # Making it to the same shape as that of question embedding\n",
    "    if i%100 ==0:\n",
    "       print(\"The count is:\",i,\"and the percentage proportion is:\",round(100*i/length,2),'%')\n",
    "images = tf.convert_to_tensor(np.array(l))\n",
    "\n",
    "images = tf.reshape(images,[length,21,300])\n",
    "img = images #Tensor containing images\n",
    "que = ques # Tensor containing question vector\n",
    "img = img/255.0 # Normalizing\n",
    "que.shape,img.shape\n",
    "ques = tf.keras.layers.Input((21,300)) # Input Model (for ques)\n",
    "images = tf.keras.layers.Input((21,300)) # Input Model (for images)\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D,\n",
    "SpatialDropout1D,Flatten,Concatenate\n",
    "''' Imagica is for the preprocessing of the image part'''\n",
    "imagica = Dense(512,activation='tanh')(images)\n",
    "imagica = Dense(512,activation='tanh')(images)\n",
    "''' quesa is for the ques layer, which means preprocessing of the question layer'''\n",
    "quesa = LSTM(512, dropout = 0.3,return_sequences = True,input_shape =\n",
    "(21,300))(ques)\n",
    "quesa = Dense(512, activation = 'relu')(quesa)\n",
    "quesa = Dropout(0.3)(quesa)\n",
    "quesa = Dense(512, activation = 'relu')(quesa)\n",
    "quesa = Dropout(0.3)(quesa)\n",
    "\n",
    "''' Concatenating both image and the question layer'''\n",
    "quesa = Concatenate()([quesa,imagica])\n",
    "quesa = Flatten()(quesa)\n",
    "out = tf.keras.layers.Dense(476,activation='softmax')(quesa) # Final output has 476\n",
    "different categories, you can check by finding length of uniquue answers :)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 0.01),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "answers = tf.keras.utils.to_categorical(ans)\n",
    "answers.shape\n",
    "model.summary()\n",
    "history=model.fit([img,que],ans,epochs = 50,batch_size=32,verbose=1)\n",
    "model.save('VQA_Model')\n",
    "prediction = tf.argmax(model.predict([img,que]),axis=1).numpy()\n",
    "count=0\n",
    "for i in range(len(ans)):\n",
    "    if(prediction[i]==ans[i]):\n",
    "        count=count+1\n",
    "acc=(count/len(ans))*100\n",
    "acc\n",
    "import json\n",
    "x = open('trainset.json','r')\n",
    "train = json.load(x)\n",
    "\n",
    "train[0]\n",
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "import pickle\n",
    "x = open('cache/trainval_label2ans.pkl','rb')\n",
    "a2lab = pickle.load(x)\n",
    "import cv2\n",
    "img_path = 'VQA_RAD Image Folder/'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "for i in range(760,770,2):\n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    image = cv2.imread(img_path+ train[i]['image_name'])\n",
    "    plt.imshow(image)\n",
    "    value = \"Actual: \"+str(train[i]['answer'])+' predicted value: '+ str(a2lab[prediction[i]])\n",
    "    plt.axis('off')\n",
    "    print(\"The Question is:\",train[i]['question'])\n",
    "    print(\"The answer is:\",value)\n",
    "    print(\"*\"*50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1abc06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
